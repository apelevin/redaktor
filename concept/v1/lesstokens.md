Ниже — текст, который можно прямо вставить в Cursor как часть техспека/правила для реализации.

---

## Как не убивать токены: два слоя контекста

В право(редактор) контекст документа всегда существует в **двух слоях**:

1. **Внутреннее состояние (Machine State)** — полный, подробный JSON.
2. **LLM-контекст (Prompt View)** — компактная выжимка только того, что нужно модели на текущем шаге.

Цель: хранить у себя максимум структуры и точности, но отправлять в LLM только минимально необходимую информацию.

---

### 1. Внутреннее состояние (Machine State)

В state/store и БД мы храним:

* Полный JSON-контекст по документу (длинные ключи, глубокие пути).
* Структуру блоков и их статусы (`must/recommended/optional`, `active/completed/inactive`).
* Историю вопросов и сырых ответов пользователя.

Требования:

* Внутренний ключ может быть длинным и точным
  (например, `apartment_rental.parties.landlord_full_name`).
* В этом слое важна однозначность, а НЕ экономия токенов.
* Все проверки “можно ли уже генерировать договор” делаются только по этому слою.

---

### 2. LLM-контекст (Prompt View)

Перед каждым вызовом LLM формируется **упрощённое представление состояния**, которое отдаётся в промпт.

Правила формирования:

1. **Отдаём только активный контекст**

   * Перед планировщиком вопросов: только список блоков с их статусами и краткими описаниями.
   * Перед генерацией вопроса: только данные по *выбранному блоку*, а не весь документ.

2. **Используем короткие имена полей**

   * Внутри LLM-части вместо длинных путей используем сокращённые идентификаторы:

     * `LL_LANDLORD_NAME` вместо `apartment_rental.parties.landlord_full_name`.
   * Маппинг “короткий ключ → настоящий путь” хранится только на бэкенде.

3. **Сжимаем длинные значения**

   * Объёмные описания (ТЗ, нестандартные условия) один раз сводим к summary.
   * В дальнейшем в промпт передаём только summary + маркер, что “детали есть в state”.

4. **Передаём не весь state, а дельту и агрегаты**

   * Вместо всего JSON отправляем:

     * агрегированную сводку по блокам (какие закрыты, какие с пробелами),
     * последние изменения (что пользователь ответил на предыдущем шаге),
     * минимальный набор полей, необходимых для решения LLM “что спрашивать дальше”.

5. **Никогда не отправляем в LLM исходный Machine State целиком**

   * Полный JSON — только для нашего движка и RAG,
   * LLM всегда видит “ограниченную проекцию” на текущую задачу.

---

### 3. Поток вызова LLM с двумя слоями контекста

1. Пользователь отвечает на вопрос.
2. Backend:

   * обновляет **Machine State** (полный JSON, блоки, ответы),
   * пересчитывает статусы блоков.
3. Backend формирует **Prompt View**:

   * выбирает активный блок,
   * подготавливает компактные данные по этому блоку (с короткими ключами и summary),
   * добавляет краткую сводку по другим блокам (только статусы, без содержимого).
4. Prompt View отправляется в LLM.
5. Ответ LLM (операции над блоками, следующий вопрос и т.п.) применяется к Machine State.

---

### 4. Принцип контроля токенов

* **Оптимизацию делаем на стороне Prompt View**, а не на состоянии.
* Любое новое место, где в LLM-контекст попадает “целый JSON”, нужно заменить на:

  * либо “данные по одному блоку”,
  * либо “агрегированную сводку”,
  * либо “summary вместо raw текста”.

---

Таким образом:

* Внутри системы мы продолжаем использовать детальный структурированный JSON для точности, отладки и генерации договора.
* LLM же на каждом шаге видит только узкую, специально подготовленную проекцию состояния, что радикально снижает расход токенов и повышает управляемость диалога.
